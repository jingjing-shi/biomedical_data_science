---
title: "models"
author: "Jingjing Shi"
date: "11/1/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE,warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load library, message=FALSE}
library(plyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(gridExtra)
library(psych)
library(corrplot)
library(ggfortify)
library(cluster)
library(factoextra)
library(VIM)
library(robustHD)
library(car)
library(e1071)
```

```{r read data}
data <- read.csv('WESAD_combined_dataset.csv')

```

#### The dataset has 44953 observations from 15 patients, and no missing values
```{r summary of data}
str(data)
summary(data)
data$ID <- as.factor(data$ID)
data_predictor <- data %>% select(EDA,ACC.2,ACC.3,ECG,EMG,Temp,Resp)
```
```{r}
data_processed <- as.data.frame(winsorize(data_predictor))
#data_processed
summary(data_processed)
corrplot.mixed(cor(data_processed),lower.col='black',tl.cex = 0.7,title = 'Corrlations of Variables',mar=c(0,0,1,0))
```


```{r models}
library(arm)
library(pROC)
library(e1071)
library(caret)

data_model <-cbind(data$lab,data_processed)
colnames(data_model) <- c('lab','EDA','ACC.2','ACC.3','ECG','EMG','Temp','Resp')
summary(data_model)
```

```{r get traindata and test data}
library(class)
library(caTools)
library(gmodels)
set.seed(123)
sample <- sample.split(data_model$lab,SplitRatio = 0.8)
train_df <- subset(data_model,sample ==TRUE)
test_df <- subset(data_model, sample==FALSE)
X_train <- train_df[,2:8]
y_train <- train_df[,1]
X_test <- test_df[,2:8]
y_test <- test_df[,1]
```


## Multilevel Classification
```{r decision tree,fig.width=12, fig.height=9}
###### CART
library(tree)
ars_cart <- tree(as.factor(lab) ~ ., data = train_df)
#ars_cart <- tree(switch ~ arsenic + assoc + dist + educnew, data = arsenic) #same result
summary(ars_cart)

plot(ars_cart)

text(ars_cart)

table(predict(ars_cart,test_df,type="class"),test_df$lab)

```

```{r random forest}
###### Bagging
library(randomForest)
ars_bagg <- randomForest(as.factor(lab) ~ ., data = train_df,importance= TRUE)
ars_bagg

varImpPlot(ars_bagg)

```


```{r}
table(predict(ars_bagg,test_df,type="class"),test_df$lab)
```


```{r boost}
###### Boosting
library(gbm)
ars_boost <-  gbm(lab ~ .,data=train_df,
               distribution="multinomial",n.trees=5000, interaction.depth=2)
summary(ars_boost)
```
```{r}
Effects <- tibble::as_tibble(gbm::summary.gbm(ars_boost, 
                                         plotit = FALSE))
Effects%>% 
  # arrange descending to get the top influencers
  dplyr::arrange(desc(rel.inf)) %>%
  # plot these data using columns
  ggplot(aes(x = forcats::fct_reorder(.f = var, 
                                      .x = rel.inf), 
             y = rel.inf, 
             fill = rel.inf)) +
  geom_col() +
  # flip
  coord_flip() +
  # format
  theme(axis.title = element_text()) + 
  xlab('Features') +
  ylab('Relative Influence') +
  ggtitle("Relative Influence Rankings")

```

```{r}
pred <- predict(ars_boost,test_df,type='response',n.trees = 5000)
labels = colnames(pred)[apply(pred, 1, which.max)]
```


```{r}
table(labels,test_df$lab)
```


```{r KNN}

knn_model <- knn(X_train,X_test,cl = y_train,k = sqrt(nrow(X_train)))
CrossTable(y_test,knn_model,prop.chisq = FALSE)
```

```{r LDA}
library(MASS)
library(ROCR)
install.packages('mda')
library(mda)
mda_model <- mda(lab~.,train_df)
mda_predict<-predict(mda_model,test_df)
mda_predict
lda_model <-lda(lab~.,train_df)
lda_predict<-predict(lda_model,test_df)$class
#view(lda_predict)
CrossTable(y_test,mda_predict,prop.chisq = FALSE)
```

```{r}
1-861/1993
1-97/1115
1-1254/2360
```

```{r svm}
svm_model <- svm(lab~.,train_df,kernel = 'radial', cost=10,gamma=1)
summary(svm_model)
plot(svm_model,train_df)
svm_predict<-predict(svm_model,test_df)
```

```{r}
svm_predict
```

## Binary Classification

Only use Stress (lab=2) and Amusement(lab=3) as outcome levels
```{r subset of data}
data_binary<-data_model %>% filter(lab ==2 | lab ==3)
data_binary <-data_binary%>% mutate (lab = case_when(lab==2~1,
                                       lab==3~0))%>%mutate(lab =as.factor(lab))
summary(data_binary)
set.seed(123)
sample <- sample.split(data_binary$lab,SplitRatio = 0.8)
train_df <- subset(data_binary,sample ==TRUE)
test_df <- subset(data_binary, sample==FALSE)
X_train <- train_df[,2:8]
y_train <- train_df[,1]
X_test <- test_df[,2:8]
y_test <- test_df[,1]
```
```{r logistic}
log_reg <- glm(lab~.,data =train_df,family=binomial(link='logit'))
vif(log_reg)
pred_log <- ifelse(predict(log_reg,test_df)>-0.5,"1","0")
## Confusion matrix
Conf_mat <- confusionMatrix(as.factor(pred_log),
                            as.factor(test_df$lab),positive = "1")
Conf_mat$table
Conf_mat$overall["Accuracy"]
Conf_mat$byClass[c("Sensitivity","Specificity")] #True positive rate and True negative rate


## ROC curve
roc(test_df$lab,predict(log_reg,test_df),plot=T,print.thres="best",legacy.axes=T,print.auc =F,col="red3")
```

```{r decision tree binary}
###### CART
ars_cart <- tree(as.factor(lab) ~ ., data = train_df)

summary(ars_cart)
plot(ars_cart)
text(ars_cart)
ars_cart
head(predict(ars_cart,test_df,type="class"))

## Confusion matrix
Conf_mat_cart <- confusionMatrix(as.factor(predict(ars_cart,test_df,type='class')),
                            as.factor(test_df$lab),positive = "1")
Conf_mat_cart$table
Conf_mat_cart$overall["Accuracy"]
Conf_mat_cart$byClass[c("Sensitivity","Specificity")] #True positive rate and True negative rate

## ROC curve
roc(test_df$lab,predict(ars_cart,test_df,type='vector')[,2],plot=T,print.thres="best",legacy.axes=T,print.auc =F,col="red3")
```

```{r random forest binary}
###### Bagging

ars_bagg <- randomForest(as.factor(lab) ~ ., data = train_df,importance=TRUE)
ars_bagg

## Confusion matrix
Conf_mat_bagg <- confusionMatrix(predict(ars_bagg,test_df,type="response"),
                                 as.factor(test_df$lab),positive = "1")
Conf_mat_bagg$table #compare to Conf_mat$table
Conf_mat_bagg$overall["Accuracy"]

Conf_mat_bagg$byClass[c("Sensitivity","Specificity")]

## ROC curve
roc(test_df$lab,predict(ars_bagg,test_df,type="prob")[,2],plot=T,print.thres="best",legacy.axes=T,print.auc =F,col="red3")
```
```{r boost binary}
###### Boosting

ars_boost <-  gbm(lab ~ .,data=train_df,distribution="multinomial",n.trees=5000, interaction.depth=2)
summary(ars_boost)
pred_boost<- predict(ars_boost,test_df,type='response',n.trees = 5000)
labels = colnames(pred_boost)[apply(pred_boost, 1, which.max)]
table(labels,test_df$lab)

roc(test_df$lab,pred_boost[,1],plot=T,print.thres="best",legacy.axes=T,print.auc =T,col="red3")
```

```{r KNN binary}


knn_model <- knn(X_train,X_test,cl = y_train,k = sqrt(nrow(X_train)))
CrossTable(y_test,knn_model,prop.chisq = FALSE)
```

```{r LDA binary}

lda_model <-lda(lab~.,train_df)
lda_predict<-predict(lda_model,test_df)$class
CrossTable(y_test,lda_predict,prop.chisq = FALSE)
```
```{r compute error rate}
(84+38+176)/(84+38+176+3224)
(289+51+341)/(289+51+341+1312)
(386+150+273)/(386+150+273+306)
(425+418+101)/(425+418+101+1416)
             
```

```{r time series}
rm(list = ls())
library(tseries)
library(forecast)
library(ggplot2)

## use patinet 8 as example
data <- read.csv('S8_final.csv')

## use EDA and ECG, create time series object
jpeg('timeseries.jpg')
par(mfrow=c(2,1))
EDA <- ts(data$EDA)
ts.plot(EDA,col='red3')

ECG <- ts(data$ECG)
ts.plot(ECG,col='blue4')
dev.off()

## look at autocorrelations

jpeg('autocorrelation_eda.jpg')
par(mfrow=c(2,1))
acf(EDA)
pacf(EDA)
dev.off()
jpeg('autocorrelation_ecg.jpg')
par(mfrow=c(2,1))
acf(ECG)
pacf(ECG)
dev.off()

#Tests for stationarity
adf_test_EDA <- adf.test(EDA,alternative = 'stationary')
print(adf_test_EDA)

adf_test_ECG<- adf.test(ECG,alternative = 'stationary')
print(adf_test_ECG)


kpss_test_EDA <- kpss.test(EDA)
print(kpss_test_EDA)

kpss_test_ECG <- kpss.test(ECG)
print(kpss_test_ECG)

auto.arima(ECG)

Model1 <- arima(ECG, order = c(3,0,4))
Model1

jpeg('forecast.jpg')
futurVal <- forecast(Model1,h=10, level=c(99.5))
autoplot(futurVal)
dev.off()
```


